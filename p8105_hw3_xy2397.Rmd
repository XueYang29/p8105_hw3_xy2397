---
title: "p8105_hw3_xy2397"
author: "Xue Yang"
date: "10/6/2018"
output: github_document
---

```{r setup, include=FALSE}


library(tidyverse)
library(ggridges)
library(patchwork)
library(hexbin)
```

# Problem 1

**Load the data**

```{r}
library(p8105.datasets)
```

**Read and clean the data**



```{r}

brfss = 
  
  # read the data "brfss_smart2010" from the p8105.datasets package
  brfss_smart2010 %>% 
  
  # clean up variable names
  janitor::clean_names() %>% 
  
  # filter the data to only focus on "Overall Health" topic
  filter(., topic == "Overall Health") %>% 
  
  # mutate responses as a factor taking levels ordered from “Excellent” to “Poor”
  mutate(., response = factor(response, levels  = c("Excellent", "Very good", "Good", "Fair", "Poor")))
  
```

**Answering the questions using this dataset**

•	In 2002, which states were observed at 7 locations?

```{r}
brfss %>% 
  
  # filter the dataset for only year 2002
  filter(year == "2002") %>% 
  # filter the dataset to distinct locations
  distinct(locationdesc, .keep_all = TRUE) %>% 
  # count the number of locations by states
  group_by(locationabbr) %>% 
  summarize(number = n()) %>% 
  # focus on the number equals to 7
  filter(number == 7)
```

By filter the full dataset for year "2002" and distinct the locationdesc, we obtain one observation from each location. When we group the data by locationabbr and count for the number of observations, we can finally get the number of locations in each states.

So we can see that states "CT", "FL" and "NC" were observed at 7 locations.

•	Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r}

  brfss %>% 
  # filter the dataset for distinct locations
  distinct(locationdesc, .keep_all = TRUE) %>% 
  # count the number of locations by year and state
  group_by(year, locationabbr) %>% 
  summarize(number = n()) %>% 
  # make the plot 
  ggplot(aes(x = year , y = number, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Spaghetti plot",
    x = "Year",
    y = "Number of observations in each state",
    caption = "Data from the p8105.datasets package"
  ) 
```

By making a plot to show the number of distinct countis in each state, we need first to distinct the locationdesc, after that count for the number of observations of each states by group the dataset by locationabbr and year. 

From what we draw in "Spaghetti plot", we can find that the number of locations in each states observed is increased from 2002 to 2010 (except for few states such as "OK" decreased from 2005 to 2009), and state "FL" increased a lot for a fast speed from 2007 to 2010.

•	Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r}
brfss %>% 
  # fiter the dataset for year "2002", "2006" and "2010", and for state "NY"
  filter(year %in% c("2002", "2006", "2010"), locationabbr == "NY") %>% 
  
  # select some useful variables
  select(1:8) %>% 
  
  # spread the data so that responses (excellent to poor) are variables taking the value of sample_zise
  spread(key = "response", value = "sample_size") %>% 
  
  # clean the name of variables
  janitor::clean_names() %>% 
  
  # create a new variable indicate the proportion of “Excellent” responses
  mutate(prop = excellent / (excellent + very_good + good + fair + poor)) %>% 
  
  # calculate the mean and standard deviation of proportion grouped by years
  group_by(year) %>% 
  summarize(mean = mean(prop, na.rm = TRUE),
            sd = sd(prop, na.rm = TRUE)) %>% 
  knitr::kable(digits = 3)
```

For the results, we can find that year 2002 has the highest mean prooportion of "Excellent" and thar year 2006 has the lowest standard deviation of rooportion of "Excellent".

•	For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
brfss %>% 
  # select some useful variables
  select(1:8) %>% 
  
  # spread the data so that responses (excellent to poor) are variables taking the value of sample_zise
  spread(key = "response", value = "sample_size") %>%
  
  # clean the name of variables
  janitor::clean_names() %>% 
  
  # create the variables for proportion in each response category
  mutate(exc_prop = excellent / (excellent + very_good + good + fair + poor),
         vgd_prop = very_good / (excellent + very_good + good + fair + poor),
         gd_prop = good / (excellent + very_good + good + fair + poor),
         fa_prop = fair / (excellent + very_good + good + fair + poor),
         pr_prop = poor / (excellent + very_good + good + fair + poor)) %>% 
  
  # count the average proportions across locations in a state in each year
  group_by(year, locationabbr) %>% 
  summarize(exc_prop_average = mean(exc_prop,na.rm = TRUE),
            vgd_prop_average = mean(vgd_prop, na.rm = TRUE),
            good_prop_average = mean(gd_prop, na.rm = TRUE),
            fair_prop_average = mean(fa_prop, na.rm = TRUE),
            poor_prop_average = mean(pr_prop, na.rm = TRUE)) %>% 
  
  # gather the response and average proportion to make it more readable
  gather(., key = "response", value = "average_prop", exc_prop_average:poor_prop_average) %>% 
  
  # mutate responses as a factor taking levels ordered from “Excellent” to “Poor”
  mutate(response = forcats::fct_relevel(response, c("exc_prop_average", "vgd_prop_average", "good_prop_average", "fair_prop_average", "poor_prop_average"))) %>% 
  
  # make a five-panel plot
  ggplot(aes(x = year, y = average_prop, color = locationabbr)) +
  geom_point()+
  facet_grid(~response) +
  labs(
    title = "Five-panel plot",
    x = "Year",
    y = "Proportion of state-level averages",
    caption = "Data from the p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "State",
    discrete = TRUE
    ) + 
  theme(legend.position = "bottom")
```

From the five-panel plot, we can find that the "poor" response has the lowest average proportion among the response and "Very good" response has the highest average proportion. And for response "Excellent", the average proportion is decreased from year 2002 to 2010.

# Problem 2

**Load and read the data**

```{r}
data("instacart")
```


**Write a short description of the dataset**

The dataset "Instacart" contains online grocery orders from instacart users. The size of the dataset is `r nrow(instacart)` rows x `r (ncol(instacart))` columns, which means that it contains `r nrow(instacart)` records of online orders about `r (ncol(instacart))` variables. There are some key variables such as id (order_id, product_id, user_id, aisle_id, department_id) for identifier, order_number for the order sequence number for this user, which the maximum is `r max(instacart$order_number)` ,the minimum is `r min(instacart$order_number)` and the median is `r median(instacart$order_number)`, order_dow for the day of the week on , order_hour_of_day for the hour of the day on which the order was placed, which the order was placed, which the maximum is `r max(instacart$order_hour_of_day)` ,the minimum is `r min(instacart$order_hour_of_day)` and the median is `r median(instacart$order_hour_of_day)`and product_name for the name of product.


**Giving illstrative examples of observations**

For example for the user whose order_id is 1, he(she) ordered online for 8 products, among those there were 4 products reordered. This order belongs to train evaluation set and this order is placed on Thursday for 10 hours, and it's 9 days since this user last order.
Then, do or answer the following (commenting on the results of each):


**Answering the questions using this dataset**

•	How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>% 
  
  # count the number of items grouped by aisle 
  group_by(aisle) %>% 
  summarize(number = n()) %>% 
  
  # focus on the top 5 high number
  top_n(5) 
```
So there are totally 134 aisles there, and fresh vegetables is the most items ordered from for 150609 orders and following is fresh fruits for 150473 orders.

•	Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
instacart %>% 
  
  # # count the number of items grouped by aisle_id
  group_by(aisle_id) %>% 
  summarise(number = n()) %>% 
  
  # make a plot shows the number of items ordered in each aisle
  ggplot(aes(x = aisle_id, y = number, color = aisle_id)) +
  geom_point()+
  labs(
    title = "Aisle plot",
    x = "Aisle_id",
    y = "Number of items ordered in each aisle",
    caption = "Data from the p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "Aisle_id",
    discrete = FALSE,
  ) +
  scale_x_continuous(
    breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80,
               90, 100, 110, 120, 130)
  ) +
  theme(legend.position = "bottom")
```

Since there are 134 different kinds of aisles, it is not readale to put the name of aisles in x-axis, so we use aisle_id instead of aisle name to represent different kinds of aisles and color the number of items differently to indicate different aisles. In this way, the plot is more readable for others.

•	Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}
instacart %>% 
  
  # filter the dataset for “baking ingredients”, “dog food care”, and “packaged vegetables fruits” aisles
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits" ) ) %>% 
  
  # count the number of items in each of these three aisles
  group_by(aisle, product_name) %>% 
  summarize(number = n()) %>% 
  
  # focus on the highest number in each aisle
  top_n(1) %>% 
  knitr::kable(digits = 3)

```

We can first filter these three types of aisles from the full dataset and then grouped the dataset by these three different aisles and product_name. When we summarize the number of each group and select the largest number of each group, we can now get the most popular item in each of the three aisles. 

From the table, we can find that the most popular item in "baking ingredients" is "Light Brown Sugar" which saled for 499 orders, the most popular item in "dog food care" is "Snack Sticks Chicken & Rice Recipe Dog Treats" which saled for 30 orders and he most popular item in "packaged vegetables fruits" is "Organic Baby Spinach" which saled for 9784 orders.

•	Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).


```{r}
instacart %>% 
  
  # filter the dataset for "Pink Lady Apples" and "Coffee Ice Cream"
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  
  # count for the mean hour of the day on each day of the week
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour_day = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  
  # spread the data make the table more readable
  spread(key = "order_dow", value = "mean_hour_day") %>% 
  knitr::kable(digits = 3)
  
```

From the table we can easily read the mean hours of the day at each day of the week users spend on product "Coffee Ice Cream" and "Pink Lady Apples". We can find that people spend most hours for "Coffee Ice Cream" at Tuesday and for "Pink Lady Apples" at Wednesday.



# Problem 3

**Load the data**

```{r}
data("ny_noaa")
ny_noaa
```

The goal is to do some exploration of this dataset. 

To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. 

This dataset "ny_noaa" provides some weather data, it contains records from 747 different weather stations for about datas from 10,957 obervational  days. The size of the dataset is `r nrow(ny_noaa)` rows x `r (ncol(ny_noaa))` columns. The key variables are id (weather station ID), date (date of observation), prcp (precipitation for tenths of mm), snow (snowfall for mm), snwd (snow depth for mm) and tmax and tmin (maximum and mimimum temperature for tenths of degrees C). At last, we calculate the proportion of missing value fro each vairable, we have  0.0561958 for prop, 0.146896 for snow, 0.2280331 for snwd, 0.4371025 for tmax and 0.4371264 for tmin.

```{r}
ny_noaa %>% 
  group_by(id) %>% 
  summarise(number = n())

ny_noaa %>% 
  group_by(date) %>% 
  summarise(number = n()) 

prcp_na =
  ny_noaa %>% 
  filter(is.na(prcp)) %>% 
  nrow()
prcp_prop = prcp_na/nrow(ny_noaa)
prcp_prop

snow_na =
  ny_noaa %>% 
  filter(is.na(snow)) %>% 
  nrow()
snow_prop = snow_na/nrow(ny_noaa)
snow_prop

snwd_na =
  ny_noaa %>% 
  filter(is.na(snwd)) %>% 
  nrow()
snwd_prop = snwd_na/nrow(ny_noaa)
snwd_prop

tmax_na =
  ny_noaa %>% 
  filter(is.na(tmax)) %>% 
  nrow()
tmax_prop = tmax_na/nrow(ny_noaa)
tmax_prop

tmin_na =
  ny_noaa %>% 
  filter(is.na(tmin)) %>% 
  nrow()
tmin_prop = tmin_na/nrow(ny_noaa)
tmin_prop
```


Then, do or answer the following (commenting on the results of each):

•	Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r}
noaa = 
  ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-")
```

```{r}
noaa %>%
  count(snow) %>% 
  top_n(5) 

```
So for snowfall, 0 is the most commonly observed value, since 0 has been observed for 2008508 times from the dataset. The second commonly observed value id 25 and it was observed for 31022 times.

•	Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
noaa %>% 
  filter(month %in% c("01", "07")) %>% 
  mutate(tmax = as.numeric(tmax)) %>% 
  group_by(id, month) %>% 
  summarize(average_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(!is.na(average_tmax)) %>% 
  ggplot(aes(x = id, y = average_tmax, color = month)) +
  geom_point(alpha = 0.5) +
  geom_line() +
  facet_grid(~month) +
  labs(
    title = "Average max temperature two-panel plot",
    x = "Weather Station",
    y = "Average max temperature across year (tenths of degrees CC)",
    caption = "Data from the p8105.datasets package"
  ) +
  theme(legend.position = "bottom")
  
```
When draw a plot for average max temperature for in January and in July in each station across years, we first delete all the missing value. From the two-panel plot we can easily find that the average temperature recorded from each station in January is lower than in July, it's obviously, since it's winter in January and summer in July. And there are also some outliers in January which may be explained that there are few countries and territories are especially cold in January.

•	Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
tmax_tmin =
  noaa %>% 
  filter(!is.na(tmax), !is.na(tmin)) %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex(bins = 300) +
  labs(
    title = "Tmax vs Tmin",
    x = "Maximum temperature",
    y = "Minimum temperature",
    caption = "Data from the p8105.datasets package"
  ) +
  scale_x_discrete(
    limits = c(-250, 250)
  ) +
  theme(legend.position = "bottom")

snowfall =
  noaa %>% 
  filter(snow %in% 1:99) %>% 
  ggplot(aes(x = snow, fill = year)) +
  geom_histogram(alpha = 0.5) +
  labs(
    title = "Distribution of snowfall",
    x = "Snowfall values (mm)",
    y = "Frequency",
    caption = "Data from the p8105.datasets package"
  ) +
  theme(legend.position = "bottom")

tmax_tmin + snowfall
```

So when we deal with the 